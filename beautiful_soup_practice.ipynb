{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "В этом задании вам необходимо реализовать парсер для сбора статистики со страниц Википедии. \n",
    "Чтобы упростить вашу задачу, необходимые страницы уже скачаны и сохранены на файловой системе в директории wiki/ \n",
    "(Например, страница https://en.wikipedia.org/wiki/Stone_Age сохранена файле wiki/Stone_Age). \n",
    "\n",
    "Парсер реализован в виде функции parse, которая принимает на вход один параметр: path_to_file — путь до файла, \n",
    "    содержащий html код страницы википедии. Гарантируется, что такой путь существует. Ваша задача — прочитать файл, \n",
    "    пройтись Beautiful Soup по статье, найти её тело (это <div id=\"bodyContent\">) и внутри него подсчитать:\n",
    "\n",
    "Количество картинок (img) с шириной (width) не меньше 200. Например: <img width=\"200\">, но не <img> и не <img width=\"199\">\n",
    "\n",
    "Количество заголовков (h1, h2, h3, h4, h5, h6), первая буква текста внутри которых соответствует заглавной букве \n",
    "E, T или C. \n",
    "Например: <h1>End</h1> или <h5><span>Contents</span></h5>, \n",
    "    но не <h1>About</h1> и не <h2>end</h2> и не <h3><span>1</span><span>End</span></h3>\n",
    "\n",
    "Длину максимальной последовательности ссылок, между которыми нет других тегов, открывающихся или закрывающихся. \n",
    "Например: <p><span><a></a></span>, <a></a>, <a></a></p> - тут 2 ссылки подряд, \n",
    "    т.к. закрывающийся span прерывает последовательность. \n",
    "    <p><a><span></span></a>, <a></a>, <a></a></p> - а тут 3 ссылки подряд, т.к. span находится внутри ссылки, \n",
    "    а не между ссылками.\n",
    "\n",
    "Количество списков (ul, ol), не вложенных в другие списки. \n",
    "Например: <ol><li></li></ol>, <ul><li><ol><li></li></ol></li></ul> - два не вложенных списка (и один вложенный)\n",
    "\n",
    "Результатом работы функции parse будет список четырех чисел, посчитанных по формулам выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import unittest\n",
    "import re\n",
    "\n",
    "\n",
    "def parse(path_to_file):    \n",
    "    \n",
    "    html = open(path_to_file, 'r', encoding='utf-8')\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    data = soup.find_all(\"div\", attrs={\"id\": \"bodyContent\"})\n",
    "    \n",
    "    regexp_img = r'(<img.+\\swidth=\"(1\\d\\d\\d\\d{0,}|[23456789]\\d\\d\\d{0,}))'\n",
    "    imgs = 0\n",
    "    for i in data[0].find_all('img'):\n",
    "        img = re.findall(regexp_img, str(i))\n",
    "        if len(img) > 0:\n",
    "            imgs += 1\n",
    "    \n",
    "    headers_ = []\n",
    "    headers_list = []\n",
    "    for n in range(7):\n",
    "        c = data[0].find_all(lambda tag: tag.name == f'h{n}' and 'C' in tag.get_text())\n",
    "        e = data[0].find_all(lambda tag: tag.name == f'h{n}' and 'E' in tag.get_text())\n",
    "        t = data[0].find_all(lambda tag: tag.name == f'h{n}' and 'T' in tag.get_text())\n",
    "        for el in [c, e, t]:\n",
    "            if len(el) > 0:\n",
    "                headers_.append(el)\n",
    "    for el in headers_:\n",
    "        for i in range(len(el)):\n",
    "            headers_list.append(el[i])\n",
    "    headers = len(headers_list)\n",
    "    \n",
    "    linkslen = 0\n",
    "    links = data[0].find_all('a')          \n",
    "    current_len = 0\n",
    "    for n in range(len(links)):\n",
    "        try:\n",
    "            if links[n].next_sibling.next_sibling != links[n+1]:\n",
    "                current_len = 0\n",
    "            else:\n",
    "                current_len += 1\n",
    "        except:\n",
    "            pass\n",
    "        if current_len > linkslen:\n",
    "            linkslen = current_len\n",
    "    linkslen += 1\n",
    "    \n",
    "    lists_data = data[0].find_all(['ul', 'ol'])\n",
    "    lists = 0\n",
    "\n",
    "    for list_ in lists_data:\n",
    "         if len(list_.find_parents(['ul', 'ol'])) == 0:\n",
    "                lists += 1\n",
    "    \n",
    "    html.close()\n",
    "    return [imgs, headers, linkslen, lists]\n",
    "\n",
    "\n",
    "class TestParse(unittest.TestCase):\n",
    "    def test_parse(self):\n",
    "        test_cases = (\n",
    "            ('wiki/Stone_Age', [13, 10, 12, 40]),\n",
    "            ('wiki/Brain', [19, 5, 25, 11]),\n",
    "            ('wiki/Artificial_intelligence', [8, 19, 13, 198]),\n",
    "            ('wiki/Python_(programming_language)', [2, 5, 17, 41]),\n",
    "            ('wiki/Spectrogram', [1, 2, 4, 7]),)\n",
    "\n",
    "        for path, expected in test_cases:\n",
    "            with self.subTest(path=path, expected=expected):\n",
    "                self.assertEqual(parse(path), expected)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = open('wiki/Stone_Age', 'r', encoding='utf-8')\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "data = soup.find_all(\"div\", attrs={\"id\": \"bodyContent\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
